{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import requests\n",
    "from urllib import parse\n",
    "from bs4 import BeautifulSoup\n",
    "from locals.output_data import exportData\n",
    "from locals.date_scrape import getDate, lastStamp, searchDate\n",
    "from locals.automate import By, Keys, EC, Action, wait_for, genBrowser, driverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up user agent\n",
    "\n",
    "header_1 = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0'\n",
    "}\n",
    "\n",
    "header_2 = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "header_3 = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/114.0'\n",
    "}\n",
    "\n",
    "header_4 = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36 OPR/100.0.0.0'\n",
    "}\n",
    "\n",
    "header_5 = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(html, data_list, base_url):\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    page_content = soup.select_one('section.content')\n",
    "\n",
    "    category = page_content.select_one('div.head span[itemprop=\"name\"]').string\n",
    "    topic_name = page_content.select_one('div.showthread__title h1').get_text(strip=True)\n",
    "\n",
    "    posts = page_content.select('div#posts div.showthread')\n",
    "\n",
    "    for post in posts:\n",
    "\n",
    "        # fixing the time scrape selector\n",
    "        date_tag = soup.select('div.threadpost-header__controls li.threadpost-header__controllink--nolink')\n",
    "        date_text = date_tag.get_text(strip=True)\n",
    "        pattern = r\"(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s\\d{1,2}\\,\\s20\\d{2}\"\n",
    "\n",
    "        match = re.search(pattern, date_text)\n",
    "\n",
    "        if match:\n",
    "            date_string = match.group()\n",
    "        else:\n",
    "            print('date find error!')\n",
    "            date_string = None\n",
    "\n",
    "        has_files = post.select_one('div.threadpost-content div.threadpost-content__attachments')\n",
    "        if has_files:\n",
    "            post_details_tag = post.select_one('div.threadpost-header div.threadpost-header__controls a[title=\"Post Permalink\"]')\n",
    "            post_url = parse.urljoin(base_url, post_details_tag['href'])\n",
    "            post_count = post_details_tag['data-postnum']\n",
    "\n",
    "            info_text = file.select_one('span.info').get_text(strip=True)\n",
    "            download_text = re.search(r\"\\b\\|[0-9\\,]+\\sdownloads\\b\", info_text).group()\n",
    "            downloads = download_text.replace('|', '').replace('downloads', '').replace(',', '').strip()\n",
    "            \n",
    "            attach_files = has_files.select('div.attachinfo')\n",
    "\n",
    "            for file in attach_files:\n",
    "                file_url = file.a['href']\n",
    "                file_name = file.a.string\n",
    "\n",
    "                data_dict = {\n",
    "                    \"file_name\": file_name,\n",
    "                    \"download_url\": file_url,\n",
    "                    \"downloads\": downloads,\n",
    "                    \"date\": date_string,\n",
    "                    \"topic_name\": topic_name,\n",
    "                    \"category\": category,\n",
    "                    \"post_url\": post_url,\n",
    "                    \"post#\": post_count\n",
    "                }\n",
    "\n",
    "                data_list.append(data_dict)\n",
    "                # print(data_dict)     \n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data to be collected\n",
    "def main():\n",
    "    error_url_list = []\n",
    "    data_list = []\n",
    "\n",
    "    session = requests.Session()\n",
    "    pages = [page for page in range(1, 85)]\n",
    "\n",
    "    for page in pages:\n",
    "        print(f\"crawling page: {page}\")\n",
    "        base_url = 'https://www.forexfactory.com'\n",
    "        extend_url = f'/thread/594890-indicator-bank?page={page}'\n",
    "        target_url = parse.urljoin(base_url, extend_url)\n",
    "\n",
    "        res = session.get(target_url, headers=header_1)\n",
    "\n",
    "        if res.status_code == 200:\n",
    "            html = res.text\n",
    "            \n",
    "            try:\n",
    "                get_data(html, data_list, base_url)\n",
    "            except:\n",
    "                break\n",
    "        else:\n",
    "            print(f'Bad Response- {res.status_code}')\n",
    "            error_url_list.append(target_url)\n",
    "    \n",
    "    return (error_url_list, data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawling page: 1\n",
      "crawling page: 2\n",
      "crawling page: 3\n",
      "crawling page: 4\n",
      "crawling page: 5\n",
      "crawling page: 6\n",
      "crawling page: 7\n",
      "crawling page: 8\n",
      "crawling page: 9\n",
      "crawling page: 10\n",
      "crawling page: 11\n",
      "crawling page: 12\n",
      "crawling page: 13\n",
      "crawling page: 14\n",
      "crawling page: 15\n",
      "crawling page: 16\n",
      "crawling page: 17\n",
      "crawling page: 18\n",
      "crawling page: 19\n",
      "crawling page: 20\n",
      "crawling page: 21\n",
      "crawling page: 22\n",
      "crawling page: 23\n",
      "crawling page: 24\n",
      "crawling page: 25\n",
      "crawling page: 26\n",
      "crawling page: 27\n",
      "crawling page: 28\n",
      "crawling page: 29\n",
      "crawling page: 30\n",
      "crawling page: 31\n",
      "crawling page: 32\n",
      "crawling page: 33\n",
      "crawling page: 34\n",
      "crawling page: 35\n",
      "crawling page: 36\n",
      "crawling page: 37\n",
      "crawling page: 38\n",
      "crawling page: 39\n",
      "crawling page: 40\n",
      "crawling page: 41\n",
      "crawling page: 42\n",
      "crawling page: 43\n",
      "crawling page: 44\n",
      "crawling page: 45\n",
      "crawling page: 46\n",
      "crawling page: 47\n",
      "crawling page: 48\n",
      "crawling page: 49\n",
      "crawling page: 50\n",
      "crawling page: 51\n",
      "crawling page: 52\n",
      "crawling page: 53\n",
      "crawling page: 54\n",
      "crawling page: 55\n",
      "crawling page: 56\n",
      "crawling page: 57\n",
      "crawling page: 58\n",
      "crawling page: 59\n",
      "crawling page: 60\n",
      "crawling page: 61\n",
      "crawling page: 62\n",
      "crawling page: 63\n",
      "crawling page: 64\n",
      "crawling page: 65\n",
      "crawling page: 66\n",
      "crawling page: 67\n",
      "crawling page: 68\n",
      "crawling page: 69\n",
      "crawling page: 70\n",
      "crawling page: 71\n",
      "crawling page: 72\n",
      "crawling page: 73\n",
      "crawling page: 74\n",
      "crawling page: 75\n",
      "crawling page: 76\n",
      "crawling page: 77\n",
      "crawling page: 78\n",
      "crawling page: 79\n",
      "crawling page: 80\n",
      "crawling page: 81\n",
      "crawling page: 82\n",
      "crawling page: 83\n",
      "crawling page: 84\n",
      "--Data Saved as JSON file!\n",
      "--Data Saved as CSV file!\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# run the crawler\n",
    "\n",
    "error_list, data_list = main()\n",
    "exportData('forex_factory', data_list)\n",
    "# print(error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
